{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras_nlp.layers import TransformerEncoder, TransformerDecoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, explained_variance_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class splitData():\n",
    "    def __init__(self):\n",
    "        self.X_train = []\n",
    "        self.X_test = []\n",
    "        self.y_train = []\n",
    "        self.y_test = []\n",
    "    \n",
    "    def split(self, x, y):\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(x, y, test_size=0.1, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['district_encoder',\\\n",
    "        'area',\\\n",
    "        'new_num_floors',\\\n",
    "        'new_bedrooms',\\\n",
    "        'houseTypes_Bán Luxury home',\\\n",
    "        'houseTypes_Bán Nhà',\\\n",
    "        'houseTypes_Bán Nhà cổ',\\\n",
    "        'houseTypes_Bán Nhà mặt phố',\\\n",
    "        'houseTypes_Bán Nhà riêng']\n",
    "\n",
    "df = pd.read_excel('HCM_data.xlsx')\n",
    "df_tranform = pd.DataFrame(data = StandardScaler().fit_transform(df.loc[:, features].values), columns = features)\n",
    "y = df['price'].values\n",
    "x = df_tranform[features].values\n",
    "data = splitData()\n",
    "data.split(x, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN seq"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple ANN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ModelRNN(keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense1 = keras.layers.Dense(64, activation='relu') #fully connected networks\n",
    "        self.dense2 = keras.layers.Dense(64, activation='relu')\n",
    "        self.dense3 = keras.layers.Dense(64, activation='relu')\n",
    "        self.dense4 = keras.layers.Dense(1, activation= 'relu')\n",
    "        \n",
    "        self.model = Sequential([self.dense1, self.dense2, self.dense3, self.dense4])\n",
    "        \n",
    "    def call(self,inputs):\n",
    "        return self.model(inputs)\n",
    "    \n",
    "    def compile(self, optimizer=\"adam\", loss=\"mse\"):\n",
    "        super().compile(optimizer, loss)\n",
    "        \n",
    "    def fit(self, data, epochs = 500, batch_size=128):\n",
    "        super().fit(data.X_train, data.y_train, epochs, batch_size)\n",
    "        \n",
    "    def evaluation(self, data):\n",
    "        y_pred = self.predict(data.X_test)\n",
    "        \n",
    "        mse = mean_squared_error(data.y_test, y_pred)\n",
    "        mae = mean_absolute_error(data.y_test, y_pred)\n",
    "        evs = explained_variance_score(data.y_test, y_pred)\n",
    "        \n",
    "        print(\"MSE: \", mse)\n",
    "        print(\"MAE: \", mae)\n",
    "        print(\"variance: \", evs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/128\n",
      "1/1 [==============================] - 0s 248ms/step - loss: 539.5642\n",
      "Epoch 2/128\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 538.2445\n",
      "Epoch 3/128\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 536.5935\n",
      "Epoch 4/128\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 534.8129\n",
      "Epoch 5/128\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 533.0001\n",
      "Epoch 6/128\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 531.1772\n",
      "Epoch 7/128\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 529.3391\n",
      "Epoch 8/128\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 527.4639\n",
      "Epoch 9/128\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 525.5210\n",
      "Epoch 10/128\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 523.4910\n",
      "Epoch 11/128\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 521.3677\n",
      "Epoch 12/128\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 519.1476\n",
      "Epoch 13/128\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 516.8373\n",
      "Epoch 14/128\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 514.4196\n",
      "Epoch 15/128\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 511.8709\n",
      "Epoch 16/128\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 509.2003\n",
      "Epoch 17/128\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 506.4164\n",
      "Epoch 18/128\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 503.5086\n",
      "Epoch 19/128\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 500.4780\n",
      "Epoch 20/128\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 497.3069\n",
      "Epoch 21/128\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 494.0220\n",
      "Epoch 22/128\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 490.6002\n",
      "Epoch 23/128\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 487.0417\n",
      "Epoch 24/128\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 483.3670\n",
      "Epoch 25/128\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 479.5733\n",
      "Epoch 26/128\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 475.6861\n",
      "Epoch 27/128\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 471.7157\n",
      "Epoch 28/128\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 467.6835\n",
      "Epoch 29/128\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 463.5952\n",
      "Epoch 30/128\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 459.4999\n",
      "Epoch 31/128\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 455.4321\n",
      "Epoch 32/128\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 451.4359\n",
      "Epoch 33/128\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 447.5423\n",
      "Epoch 34/128\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 443.7997\n",
      "Epoch 35/128\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 440.2158\n",
      "Epoch 36/128\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 436.8577\n",
      "Epoch 37/128\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 433.7876\n",
      "Epoch 38/128\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 431.0513\n",
      "Epoch 39/128\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 428.6612\n",
      "Epoch 40/128\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 426.6106\n",
      "Epoch 41/128\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 424.9100\n",
      "Epoch 42/128\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 423.5426\n",
      "Epoch 43/128\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 422.4311\n",
      "Epoch 44/128\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 421.5050\n",
      "Epoch 45/128\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 420.6920\n",
      "Epoch 46/128\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 419.9299\n",
      "Epoch 47/128\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 419.1451\n",
      "Epoch 48/128\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 418.3040\n",
      "Epoch 49/128\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 417.3972\n",
      "Epoch 50/128\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 416.4053\n",
      "Epoch 51/128\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 415.3353\n",
      "Epoch 52/128\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 414.2130\n",
      "Epoch 53/128\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 413.0730\n",
      "Epoch 54/128\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 411.9455\n",
      "Epoch 55/128\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 410.8638\n",
      "Epoch 56/128\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 409.8503\n",
      "Epoch 57/128\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 408.9273\n",
      "Epoch 58/128\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 408.1035\n",
      "Epoch 59/128\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 407.3776\n",
      "Epoch 60/128\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 406.7430\n",
      "Epoch 61/128\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 406.1865\n",
      "Epoch 62/128\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 405.6902\n",
      "Epoch 63/128\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 405.2389\n",
      "Epoch 64/128\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 404.8202\n",
      "Epoch 65/128\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 404.4230\n",
      "Epoch 66/128\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 404.0408\n",
      "Epoch 67/128\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 403.6666\n",
      "Epoch 68/128\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 403.2974\n",
      "Epoch 69/128\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 402.9287\n",
      "Epoch 70/128\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 402.5576\n",
      "Epoch 71/128\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 402.1879\n",
      "Epoch 72/128\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 401.8283\n",
      "Epoch 73/128\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 401.4775\n",
      "Epoch 74/128\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 401.1414\n",
      "Epoch 75/128\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 400.8239\n",
      "Epoch 76/128\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 400.5179\n",
      "Epoch 77/128\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 400.2208\n",
      "Epoch 78/128\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 399.9319\n",
      "Epoch 79/128\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 399.6494\n",
      "Epoch 80/128\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 399.3771\n",
      "Epoch 81/128\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 399.1091\n",
      "Epoch 82/128\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 398.8451\n",
      "Epoch 83/128\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 398.5789\n",
      "Epoch 84/128\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 398.3161\n",
      "Epoch 85/128\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 398.0500\n",
      "Epoch 86/128\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 397.7786\n",
      "Epoch 87/128\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 397.5037\n",
      "Epoch 88/128\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 397.2283\n",
      "Epoch 89/128\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 396.9529\n",
      "Epoch 90/128\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 396.6809\n",
      "Epoch 91/128\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 396.4133\n",
      "Epoch 92/128\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 396.1474\n",
      "Epoch 93/128\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 395.8813\n",
      "Epoch 94/128\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 395.6158\n",
      "Epoch 95/128\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 395.3494\n",
      "Epoch 96/128\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 395.0834\n",
      "Epoch 97/128\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 394.8160\n",
      "Epoch 98/128\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 394.5465\n",
      "Epoch 99/128\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 394.2765\n",
      "Epoch 100/128\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 394.0019\n",
      "Epoch 101/128\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 393.7242\n",
      "Epoch 102/128\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 393.4385\n",
      "Epoch 103/128\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 393.1526\n",
      "Epoch 104/128\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 392.8665\n",
      "Epoch 105/128\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 392.5769\n",
      "Epoch 106/128\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 392.2865\n",
      "Epoch 107/128\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 391.9963\n",
      "Epoch 108/128\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 391.7014\n",
      "Epoch 109/128\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 391.4037\n",
      "Epoch 110/128\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 391.1057\n",
      "Epoch 111/128\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 390.8038\n",
      "Epoch 112/128\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 390.4952\n",
      "Epoch 113/128\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 390.1840\n",
      "Epoch 114/128\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 389.8697\n",
      "Epoch 115/128\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 389.5523\n",
      "Epoch 116/128\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 389.2270\n",
      "Epoch 117/128\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 388.8988\n",
      "Epoch 118/128\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 388.5713\n",
      "Epoch 119/128\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 388.2366\n",
      "Epoch 120/128\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 387.8977\n",
      "Epoch 121/128\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 387.5533\n",
      "Epoch 122/128\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 387.2052\n",
      "Epoch 123/128\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 386.8508\n",
      "Epoch 124/128\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 386.4883\n",
      "Epoch 125/128\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 386.1194\n",
      "Epoch 126/128\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 385.7388\n",
      "Epoch 127/128\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 385.3484\n",
      "Epoch 128/128\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 384.9504\n",
      "15/15 [==============================] - 0s 687us/step\n",
      "MSE:  463.92430332012583\n",
      "MAE:  5.543178435643514\n",
      "variance:  0.14103822253340992\n"
     ]
    }
   ],
   "source": [
    "model_rnn = ModelRNN()\n",
    "model_rnn.compile(optimizer='adam', loss='mse')\n",
    "model_rnn.fit(data, epochs=10000, batch_size =128)\n",
    "model_rnn.evaluation(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(keras.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Sequential([\n",
    "            keras.layers.Dense(128, activation='relu'),\n",
    "            keras.layers.Dense(128, activation='relu'),\n",
    "            keras.layers.Dense(64, activation='relu'),\n",
    "        ])\n",
    "\n",
    "        self.decoder = Sequential([\n",
    "            keras.layers.Dense(64, activation='relu'),\n",
    "            keras.layers.Dense(128, activation='relu'),\n",
    "            keras.layers.Dense(128, activation='relu'),\n",
    "            keras.layers.Dense(128, activation='relu')\n",
    "        ])\n",
    "        self.dense = Dense(1)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        encoder_outputs = self.encoder(inputs)\n",
    "        decoder_outputs = self.decoder(encoder_outputs)\n",
    "        outputs = self.dense(decoder_outputs)\n",
    "        return outputs\n",
    "    \n",
    "    def compile(self, optimizer='adam', loss='mse'):\n",
    "        super().compile(optimizer, loss)\n",
    "        \n",
    "    def fit(self, x_train, y_train, epochs=1000, batch_size =128):\n",
    "        super().fit(x_train, y_train, epochs, batch_size)\n",
    "    \n",
    "    def evaluation(self, data):\n",
    "        y_pred = self.predict(data.X_test)\n",
    "        \n",
    "        mse = mean_squared_error(data.y_test, y_pred)\n",
    "        mae = mean_absolute_error(data.y_test, y_pred)\n",
    "        evs = explained_variance_score(data.y_test, y_pred)\n",
    "        \n",
    "        print(\"MSE: \", mse)\n",
    "        print(\"MAE: \", mae)\n",
    "        print(\"variance: \", evs)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/128\n",
      "9/9 [==============================] - 1s 3ms/step - loss: 527.4604\n",
      "Epoch 2/128\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 444.6162\n",
      "Epoch 3/128\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 432.5425\n",
      "Epoch 4/128\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 418.3723\n",
      "Epoch 5/128\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 407.1867\n",
      "Epoch 6/128\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 398.5597\n",
      "Epoch 7/128\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 401.4498\n",
      "Epoch 8/128\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 395.7618\n",
      "Epoch 9/128\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 392.2677\n",
      "Epoch 10/128\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 390.8188\n",
      "Epoch 11/128\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 386.2924\n",
      "Epoch 12/128\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 384.7236\n",
      "Epoch 13/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 373.2153\n",
      "Epoch 14/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 368.9081\n",
      "Epoch 15/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 369.0862\n",
      "Epoch 16/128\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 372.2315\n",
      "Epoch 17/128\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 368.5113\n",
      "Epoch 18/128\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 363.0372\n",
      "Epoch 19/128\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 352.6432\n",
      "Epoch 20/128\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 350.3208\n",
      "Epoch 21/128\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 345.8730\n",
      "Epoch 22/128\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 339.9641\n",
      "Epoch 23/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 335.8299\n",
      "Epoch 24/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 334.9860\n",
      "Epoch 25/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 333.0798\n",
      "Epoch 26/128\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 328.7667\n",
      "Epoch 27/128\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 325.3691\n",
      "Epoch 28/128\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 325.6322\n",
      "Epoch 29/128\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 323.5797\n",
      "Epoch 30/128\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 321.1537\n",
      "Epoch 31/128\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 320.9258\n",
      "Epoch 32/128\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 321.1921\n",
      "Epoch 33/128\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 319.9983\n",
      "Epoch 34/128\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 320.4757\n",
      "Epoch 35/128\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 323.8981\n",
      "Epoch 36/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 320.0197\n",
      "Epoch 37/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 319.6241\n",
      "Epoch 38/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 317.8807\n",
      "Epoch 39/128\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 315.1343\n",
      "Epoch 40/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 315.9559\n",
      "Epoch 41/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 318.8279\n",
      "Epoch 42/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 314.0385\n",
      "Epoch 43/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 312.8418\n",
      "Epoch 44/128\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 312.5155\n",
      "Epoch 45/128\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 312.3309\n",
      "Epoch 46/128\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 311.9624\n",
      "Epoch 47/128\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 310.1021\n",
      "Epoch 48/128\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 308.5680\n",
      "Epoch 49/128\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 310.2558\n",
      "Epoch 50/128\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 308.8011\n",
      "Epoch 51/128\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 308.0007\n",
      "Epoch 52/128\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 308.1571\n",
      "Epoch 53/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 310.7031\n",
      "Epoch 54/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 310.7899\n",
      "Epoch 55/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 306.0107\n",
      "Epoch 56/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 314.9667\n",
      "Epoch 57/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 313.7701\n",
      "Epoch 58/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 308.1755\n",
      "Epoch 59/128\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 313.3294\n",
      "Epoch 60/128\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 313.5281\n",
      "Epoch 61/128\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 314.4624\n",
      "Epoch 62/128\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 317.0652\n",
      "Epoch 63/128\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 332.6370\n",
      "Epoch 64/128\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 318.1651\n",
      "Epoch 65/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 310.7422\n",
      "Epoch 66/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 310.0821\n",
      "Epoch 67/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 306.2349\n",
      "Epoch 68/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 304.4619\n",
      "Epoch 69/128\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 300.9855\n",
      "Epoch 70/128\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 300.2817\n",
      "Epoch 71/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 300.8258\n",
      "Epoch 72/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 302.3989\n",
      "Epoch 73/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 297.6516\n",
      "Epoch 74/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 299.7896\n",
      "Epoch 75/128\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 313.9149\n",
      "Epoch 76/128\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 304.6083\n",
      "Epoch 77/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 312.9045\n",
      "Epoch 78/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 306.1083\n",
      "Epoch 79/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 300.5267\n",
      "Epoch 80/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 299.3939\n",
      "Epoch 81/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 297.2297\n",
      "Epoch 82/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 298.0257\n",
      "Epoch 83/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 296.9637\n",
      "Epoch 84/128\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 292.7228\n",
      "Epoch 85/128\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 292.6836\n",
      "Epoch 86/128\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 293.4501\n",
      "Epoch 87/128\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 300.0387\n",
      "Epoch 88/128\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 294.3076\n",
      "Epoch 89/128\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 291.0536\n",
      "Epoch 90/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 292.7140\n",
      "Epoch 91/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 292.3346\n",
      "Epoch 92/128\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 295.3446\n",
      "Epoch 93/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 294.0094\n",
      "Epoch 94/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 289.7341\n",
      "Epoch 95/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 285.8934\n",
      "Epoch 96/128\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 285.3169\n",
      "Epoch 97/128\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 289.3331\n",
      "Epoch 98/128\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 283.7271\n",
      "Epoch 99/128\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 286.3019\n",
      "Epoch 100/128\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 284.7628\n",
      "Epoch 101/128\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 281.3885\n",
      "Epoch 102/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 280.7183\n",
      "Epoch 103/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 281.4101\n",
      "Epoch 104/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 302.0901\n",
      "Epoch 105/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 293.4885\n",
      "Epoch 106/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 293.7283\n",
      "Epoch 107/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 291.4433\n",
      "Epoch 108/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 282.4521\n",
      "Epoch 109/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 279.9576\n",
      "Epoch 110/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 275.6647\n",
      "Epoch 111/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 274.1616\n",
      "Epoch 112/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 274.5897\n",
      "Epoch 113/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 271.5312\n",
      "Epoch 114/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 273.3036\n",
      "Epoch 115/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 274.2008\n",
      "Epoch 116/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 280.4748\n",
      "Epoch 117/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 272.9435\n",
      "Epoch 118/128\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 265.7145\n",
      "Epoch 119/128\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 265.4552\n",
      "Epoch 120/128\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 269.5295\n",
      "Epoch 121/128\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 268.9581\n",
      "Epoch 122/128\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 275.6420\n",
      "Epoch 123/128\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 286.6777\n",
      "Epoch 124/128\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 295.4520\n",
      "Epoch 125/128\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 280.5418\n",
      "Epoch 126/128\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 259.0318\n",
      "Epoch 127/128\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 262.9705\n",
      "Epoch 128/128\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 292.7551\n",
      "15/15 [==============================] - 0s 725us/step\n",
      "MSE:  446.1833048511941\n",
      "MAE:  5.219858334408012\n",
      "variance:  0.17541580027347092\n"
     ]
    }
   ],
   "source": [
    "model = Seq2Seq()\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.fit(data.X_train, data.y_train, epochs=500, batch_size =128)\n",
    "model.evaluation(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Transformer(keras.Model):\n",
    "    \n",
    "    def __init__(self, num_layers, d_model, num_heads, dropout_rate):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.encoder = TransformerEncoder(num_layers, d_model, num_heads, dropout_rate)\n",
    "        self.decoder = TransformerDecoder(num_layers, d_model, num_heads, dropout_rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        encoder_outputs = self.encoder(inputs, training=training)\n",
    "        decoder_outputs = self.decoder(encoder_outputs, training=training)\n",
    "        \n",
    "        return decoder_outputs\n",
    "\n",
    "class TransformerEncoder(keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, num_layers, d_model, num_heads, dropout_rate):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        \n",
    "        self.attention_layers = []\n",
    "        for _ in range(num_layers):\n",
    "            self.attention_layers.append(\n",
    "                keras.layers.MultiHeadAttention(\\\n",
    "                    num_heads = num_heads,\\\n",
    "                    key_dim = d_model,\\\n",
    "                    dropout = dropout_rate\\\n",
    "                )\\\n",
    "            )\n",
    "        self.feed_forward_layers = []\n",
    "        for _ in range(num_layers):\n",
    "            self.feed_forward_layers.append(\n",
    "                keras.layers.Dense(\\\n",
    "                    d_model,\\\n",
    "                    activation = \"relu\"\\\n",
    "                )\\\n",
    "            )\n",
    "            \n",
    "    def call(self, inputs, training=False):\n",
    "        for attention_layer in self.attention_layers:\n",
    "            inputs = attention_layer(inputs, inputs, training=training)\n",
    "            inputs = keras.layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "            \n",
    "        for feed_forward_layer in self.feed_forward_layers:\n",
    "            inputs = feed_forward_layer(inputs)\n",
    "            inputs = keras.layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "            \n",
    "        return inputs\n",
    "\n",
    "class TransformerDecoder(keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dropout_rate):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        \n",
    "        self.attention_layers = []\n",
    "        for _ in range(num_layers):\n",
    "            self.attention_layers.append(\n",
    "                keras.layers.MultiHeadAttention(\n",
    "                    num_heads = num_heads,\n",
    "                    key_dim = d_model,\n",
    "                    dropout = dropout_rate\n",
    "                )\n",
    "            )\n",
    "        self.feed_forward_layers = []\n",
    "        for _ in range(num_layers):\n",
    "            self.feed_forward_layers.append(\\\n",
    "                keras.layers.Dense(\\\n",
    "                    d_model,\\\n",
    "                    activation=\"relu\"\\\n",
    "                )\\\n",
    "            )\n",
    "            \n",
    "        self.final_layer = keras.layers.Dense(\n",
    "            1, # kích thước output\n",
    "            activation=\"softmax\" \n",
    "        )\n",
    "        \n",
    "    def call(self, inputs, encoder_outputs, training=False):\n",
    "        for attention_layer in self.attention_layers:\n",
    "            inputs = attention_layer(inputs, encoder_outputs, training=training)\n",
    "            inputs = keras.layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "        for feed_forward_layer in self.feed_forward_layers:\n",
    "            inputs = feed_forward_layer(inputs)\n",
    "            inputs = keras.layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "        outputs = self.final_layer(inputs)\n",
    "        return outputs\n",
    "\n",
    "class TransformerModel(keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder_input = Input(shape=(None, 32))\n",
    "        \n",
    "        self.encoder = TransformerEncoder(\n",
    "            num_encoder_layers=6,\n",
    "            num_attention_heads=8,\n",
    "            d_model=128,\n",
    "            dropout=0.1,\n",
    "        )\n",
    "        self.encoder_output = encoder(encoder_input)\n",
    "        \n",
    "        self.decoder_input = Input(shape=(None, 32))\n",
    "        self.decoder = TransformerDecoder(\n",
    "            num_decoder_layers=6,\n",
    "            num_attention_heads=8,\n",
    "            d_model=128,\n",
    "            dropout=0.1,\n",
    "        )\n",
    "        self.decoder_output = decoder(decoder_input, encoder_output)\n",
    "        self.model = Model(self.encoder_input, self.decoder_output)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self.model(inputs)\n",
    "    \n",
    "    def fit(self, x_train, y_train, epochs=1000, batch_size =128):\n",
    "        super().fit(x_train, y_train, epochs, batch_size)\n",
    "    \n",
    "    def evaluation(self, data):\n",
    "        y_pred = self.predict(data.X_test)\n",
    "        \n",
    "        mse = mean_squared_error(data.y_test, y_pred)\n",
    "        mae = mean_absolute_error(data.y_test, y_pred)\n",
    "        evs = explained_variance_score(data.y_test, y_pred)\n",
    "        \n",
    "        print(\"MSE: \", mse)\n",
    "        print(\"MAE: \", mae)\n",
    "        print(\"variance: \", evs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TransformerEncoder.__init__() got an unexpected keyword argument 'num_encoder_layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_tran \u001b[39m=\u001b[39m TransformerModel()\n\u001b[1;32m      2\u001b[0m model_tran\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmse\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m model_tran\u001b[39m.\u001b[39mfit(data, epochs\u001b[39m=\u001b[39m\u001b[39m500\u001b[39m, batch_size \u001b[39m=\u001b[39m\u001b[39m128\u001b[39m)\n",
      "Cell \u001b[0;32mIn[54], line 92\u001b[0m, in \u001b[0;36mTransformerModel.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m     90\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder_input \u001b[39m=\u001b[39m Input(shape\u001b[39m=\u001b[39m(\u001b[39mNone\u001b[39;00m, \u001b[39m32\u001b[39m))\n\u001b[0;32m---> 92\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder \u001b[39m=\u001b[39m TransformerEncoder(\n\u001b[1;32m     93\u001b[0m     num_encoder_layers\u001b[39m=\u001b[39;49m\u001b[39m6\u001b[39;49m,\n\u001b[1;32m     94\u001b[0m     num_attention_heads\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m,\n\u001b[1;32m     95\u001b[0m     d_model\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m,\n\u001b[1;32m     96\u001b[0m     dropout\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m,\n\u001b[1;32m     97\u001b[0m )\n\u001b[1;32m     98\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder_output \u001b[39m=\u001b[39m encoder(encoder_input)\n\u001b[1;32m    100\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder_input \u001b[39m=\u001b[39m Input(shape\u001b[39m=\u001b[39m(\u001b[39mNone\u001b[39;00m, \u001b[39m32\u001b[39m))\n",
      "\u001b[0;31mTypeError\u001b[0m: TransformerEncoder.__init__() got an unexpected keyword argument 'num_encoder_layers'"
     ]
    }
   ],
   "source": [
    "model_tran = TransformerModel()\n",
    "model_tran.compile(optimizer='adam', loss='mse')\n",
    "model_tran.fit(data, epochs=500, batch_size =128)\n",
    "model_tran.evaluation(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
